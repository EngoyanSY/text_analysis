{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>А-02-21 Енгоян Сергей ЛР 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.windows.x', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=28, categories=categories, remove=remove)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=28, categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем по одному тексту каждой их 3 категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.windows.x \n",
      "I think the original post was searching for existing implementations of\n",
      "f.i. Gouroud-shading of triangles. This is fairly complex to do with plain\n",
      "X. Simpler shading models are implemented already, f.i. in x3d (ask archie\n",
      "where to get the latest version).\n",
      "For Gouroud, a fast implementation will be possible utilizing some extension\n",
      "only, either MIT-SHM to do the shade in an image and fast update the window\n",
      "with it, or PEX/OpenGL which should be able to shade themselves. The portable\n",
      "'vanilla X' way would be to shade in a normal XImage and use XPutImage(),\n",
      "what would be good enough to do static things as f.i. fractal landscapes\n",
      "or such stuff.\n",
      "\n",
      "To speak about POVRay, the X previewer that comes with the original source\n",
      "package is not that good, especially in speed, protocol-friendlyness and\n",
      "ICCCM compliance. Have a look on x256q, my own preview code. It is on\n",
      "\n",
      "141.76.1.11:pub/gfx/ray/misc/x256q/\n",
      "\n",
      "The README states the points where it is better than xwindow.c from\n",
      "POVRay 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[5]], twenty_train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey \n",
      "\n",
      "\n",
      "Well, it seems that the Habs have been much talked-about of late, so here's my\n",
      "$0.02.  These guys have absolutely no concept of how to play in front of the\n",
      "damn net!!!  Watch them in the offensive zone, especially on the powerplay. \n",
      "Damphousse or Lebeau will skate all over the bloody zone, maybe pass to the\n",
      "point, get it back, skate some more, pass it around....BUT WHERE'S THE SHOT??!\n",
      "Answer: the shot is totally useless because they lack a forward who stands\n",
      "in front of the net a la` Neely, Shanahan, Tocchet, etc etc.  Too bad \n",
      "Demers won't put Dipietro or LeClair on the powerplay more often.  Dammit,\n",
      "even Ewen would at least cause some disruptions.  Montreal desperately needs\n",
      "a power forward with some talent, IMO.\n",
      "\n",
      "Then watch them in their own zone. Patrick Roy is screened on everything. Say\n",
      "what you want about his performance; IMNSHO he cannot stop what he cannot see. \n",
      "And Montreal's defence does a miserable job of clearing the front of the net. \n",
      "Last night against Washington Roy played a *great* game.  The first goal came\n",
      "on the most ridiculous goalmouth scramble I've seen in a long time, and he\n",
      "didn't have a hope in hell of stopping the shot.  The second goal came on a\n",
      "deflection of a shot he only partially saw anyway.  Pathetic defence.  The\n",
      "third goal was EN.\n",
      "\n",
      "No wonder he gets pissed off at his defencemen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[1]], twenty_train.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball Amazingly, pitchers, no matter how good their mechanics, are\n",
      "not machines.  Cy Young winners don't pitch in a vaccuum, unaware\n",
      "of how their offenses are doing.\n",
      "\n",
      "\tThe Braves' pitching staff is already showing signs of\n",
      "cracking under the strain of knowing they're not going to get many\n",
      "(if any) runs.  Unfortunately, the Braves' pitchers were so bad for so\n",
      "long that the organization put so much stress (and I mean *stress*)\n",
      "on pitching that they completely ignored hitting.\n",
      "\n",
      "\tThe Braves right now are looking woefully similar to the Braves of\n",
      "the mid-seventies.  Heaven help us.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[13]], twenty_train.data[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Стемминг</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "stem_train = []\n",
    "stem_test = []\n",
    "for text in twenty_train.data:\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ''\n",
    "    for word in nltk_tokens:\n",
    "        line += ' ' + porter_stemmer.stem(word)\n",
    "    stem_train.append(line)\n",
    "\n",
    "for text in twenty_test.data:\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ''\n",
    "    for word in nltk_tokens:\n",
    "        line += ' ' + porter_stemmer.stem(word)\n",
    "    stem_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, it seems that everyone else in canada was treated to the \n",
      "REAL ABC telecast while only the people on ROGERS TV in Surrey BC \n",
      "were treated to two channels with Don \"I stink as a Commentator\" Whitman\n",
      "doing the play-by-play.\n",
      " ok , it seem that everyon els in canada wa treat to the real abc telecast while onli the peopl on roger tv in surrey bc were treat to two channel with don `` i stink as a comment '' whitman do the play-by-play .\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0])\n",
    "print(stem_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Векторизация</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_1 = CountVectorizer(max_features=10000)\n",
    "\n",
    "train_data = vect_1.fit_transform(twenty_train.data)\n",
    "test_data = vect_1.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', np.int64(15749))\n",
      "('to', np.int64(7012))\n",
      "('and', np.int64(5437))\n",
      "('of', np.int64(5008))\n",
      "('in', np.int64(4583))\n",
      "('is', np.int64(3967))\n",
      "('that', np.int64(3001))\n",
      "('for', np.int64(2833))\n",
      "('it', np.int64(2597))\n",
      "('on', np.int64(2307))\n",
      "('you', np.int64(2059))\n",
      "('this', np.int64(2006))\n",
      "('be', np.int64(1979))\n",
      "('with', np.int64(1708))\n",
      "('have', np.int64(1652))\n",
      "('are', np.int64(1634))\n",
      "('he', np.int64(1524))\n",
      "('if', np.int64(1522))\n",
      "('as', np.int64(1453))\n",
      "('but', np.int64(1438))\n"
     ]
    }
   ],
   "source": [
    "x_1 = list(zip(vect_1.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "def SortbyTF(inputStr):\n",
    "    return inputStr[1]\n",
    "x_1.sort(key=SortbyTF, reverse = True)\n",
    "\n",
    "for _ in x_1[:20]:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Частые термины для категории: comp.windows.x\n",
      "the: 6750\n",
      "to: 3486\n",
      "and: 2447\n",
      "of: 2296\n",
      "is: 2269\n",
      "in: 1848\n",
      "for: 1394\n",
      "it: 1261\n",
      "on: 1141\n",
      "that: 1130\n",
      "this: 1125\n",
      "you: 1066\n",
      "be: 956\n",
      "are: 893\n",
      "if: 836\n",
      "with: 821\n",
      "or: 778\n",
      "can: 756\n",
      "an: 730\n",
      "not: 699\n",
      "\n",
      "Частые термины для категории: rec.sport.baseball\n",
      "the: 3508\n",
      "to: 1481\n",
      "and: 1312\n",
      "of: 1142\n",
      "in: 1114\n",
      "that: 882\n",
      "is: 842\n",
      "he: 738\n",
      "for: 580\n",
      "it: 543\n",
      "have: 494\n",
      "was: 466\n",
      "but: 451\n",
      "be: 448\n",
      "you: 443\n",
      "on: 441\n",
      "this: 416\n",
      "they: 409\n",
      "at: 404\n",
      "with: 393\n",
      "\n",
      "Частые термины для категории: rec.sport.hockey\n",
      "the: 5491\n",
      "to: 2045\n",
      "and: 1678\n",
      "in: 1621\n",
      "of: 1570\n",
      "that: 989\n",
      "for: 859\n",
      "is: 856\n",
      "it: 793\n",
      "he: 776\n",
      "was: 731\n",
      "on: 725\n",
      "be: 575\n",
      "you: 550\n",
      "at: 547\n",
      "but: 532\n",
      "have: 512\n",
      "with: 494\n",
      "team: 478\n",
      "as: 466\n"
     ]
    }
   ],
   "source": [
    "category_names = twenty_train.target_names\n",
    "category_indices = twenty_train.target\n",
    "\n",
    "word_freq_pairs_1 = {}\n",
    "word_list_1 = {0:[], 1:[], 2:[]}\n",
    "\n",
    "\n",
    "for cat_idx, cat_name in enumerate(twenty_train.target_names):\n",
    "    print(f\"\\nЧастые термины для категории: {cat_name}\")\n",
    "\n",
    "    cat_mask = (category_indices == cat_idx)\n",
    "    cat_data = train_data[cat_mask]\n",
    "    \n",
    "    word_frequencies = np.ravel(cat_data.sum(axis=0))\n",
    "    word_freq_pairs_1[cat_idx] = list(zip(vect_1.get_feature_names_out(), word_frequencies))\n",
    "    word_freq_pairs_1[cat_idx].sort(key=SortbyTF, reverse=True)\n",
    "\n",
    "    for word, freq in word_freq_pairs_1[cat_idx]:\n",
    "        if freq != 0:\n",
    "            word_list_1[cat_idx].append(word)\n",
    "\n",
    "    for word, freq in word_freq_pairs_1[cat_idx][:20]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсечение стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('team', np.int64(679))\n",
      "('game', np.int64(633))\n",
      "('year', np.int64(629))\n",
      "('file', np.int64(586))\n",
      "('like', np.int64(582))\n",
      "('10', np.int64(580))\n",
      "('window', np.int64(573))\n",
      "('edu', np.int64(544))\n",
      "('use', np.int64(512))\n",
      "('don', np.int64(502))\n",
      "('just', np.int64(480))\n",
      "('time', np.int64(466))\n",
      "('new', np.int64(437))\n",
      "('good', np.int64(434))\n",
      "('think', np.int64(429))\n",
      "('play', np.int64(427))\n",
      "('season', np.int64(424))\n",
      "('program', np.int64(417))\n",
      "('games', np.int64(416))\n",
      "('11', np.int64(415))\n",
      "\n",
      "Частые термины для категории: comp.windows.x\n",
      "file: 579\n",
      "window: 571\n",
      "use: 459\n",
      "program: 412\n",
      "server: 385\n",
      "edu: 377\n",
      "motif: 356\n",
      "widget: 354\n",
      "entry: 351\n",
      "output: 315\n",
      "available: 306\n",
      "com: 274\n",
      "set: 274\n",
      "using: 269\n",
      "mit: 254\n",
      "application: 253\n",
      "like: 247\n",
      "information: 245\n",
      "sun: 240\n",
      "does: 232\n",
      "\n",
      "Частые термины для категории: rec.sport.baseball\n",
      "year: 310\n",
      "game: 204\n",
      "good: 200\n",
      "team: 195\n",
      "think: 189\n",
      "don: 186\n",
      "00: 175\n",
      "just: 161\n",
      "like: 153\n",
      "games: 149\n",
      "better: 140\n",
      "baseball: 137\n",
      "hit: 137\n",
      "runs: 137\n",
      "players: 135\n",
      "time: 131\n",
      "02: 125\n",
      "won: 124\n",
      "league: 118\n",
      "03: 116\n",
      "\n",
      "Частые термины для категории: rec.sport.hockey\n",
      "team: 478\n",
      "game: 423\n",
      "10: 406\n",
      "hockey: 365\n",
      "25: 352\n",
      "play: 343\n",
      "55: 340\n",
      "season: 312\n",
      "11: 311\n",
      "12: 282\n",
      "16: 273\n",
      "games: 265\n",
      "period: 257\n",
      "14: 256\n",
      "15: 252\n",
      "18: 247\n",
      "20: 242\n",
      "nhl: 236\n",
      "13: 233\n",
      "year: 228\n"
     ]
    }
   ],
   "source": [
    "vect_2 = CountVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "train_data = vect_2.fit_transform(twenty_train.data)\n",
    "test_data = vect_2.transform(twenty_test.data)\n",
    "\n",
    "x_2 = list(zip(vect_2.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "def SortbyTF(inputStr):\n",
    "    return inputStr[1]\n",
    "x_2.sort(key=SortbyTF, reverse = True)\n",
    "\n",
    "for _ in x_2[:20]:\n",
    "    print(_)\n",
    "\n",
    "category_names = twenty_train.target_names\n",
    "category_indices = twenty_train.target\n",
    "\n",
    "word_freq_pairs_2 = {}\n",
    "word_list_2 = {0:[], 1:[], 2:[]}\n",
    "\n",
    "\n",
    "for cat_idx, cat_name in enumerate(twenty_train.target_names):\n",
    "    print(f\"\\nЧастые термины для категории: {cat_name}\")\n",
    "\n",
    "    cat_mask = (category_indices == cat_idx)\n",
    "    cat_data = train_data[cat_mask]\n",
    "    \n",
    "    word_frequencies = np.ravel(cat_data.sum(axis=0))\n",
    "    word_freq_pairs_2[cat_idx] = list(zip(vect_2.get_feature_names_out(), word_frequencies))\n",
    "    word_freq_pairs_2[cat_idx].sort(key=SortbyTF, reverse=True)\n",
    "\n",
    "    for word, freq in word_freq_pairs_2[cat_idx]:\n",
    "        if freq != 0:\n",
    "            word_list_2[cat_idx].append(word)\n",
    "\n",
    "    for word, freq in word_freq_pairs_2[cat_idx][:20]:\n",
    "        print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пункт А для выборки со стеммингом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', np.int64(15747))\n",
      "('to', np.int64(7012))\n",
      "('and', np.int64(5437))\n",
      "('of', np.int64(5008))\n",
      "('in', np.int64(4586))\n",
      "('is', np.int64(4065))\n",
      "('that', np.int64(3003))\n",
      "('for', np.int64(2833))\n",
      "('it', np.int64(2787))\n",
      "('on', np.int64(2310))\n",
      "('be', np.int64(2168))\n",
      "('you', np.int64(2059))\n",
      "('thi', np.int64(2006))\n",
      "('have', np.int64(1795))\n",
      "('with', np.int64(1708))\n",
      "('are', np.int64(1677))\n",
      "('he', np.int64(1524))\n",
      "('if', np.int64(1522))\n",
      "('not', np.int64(1467))\n",
      "('wa', np.int64(1456))\n",
      "\n",
      "Частые термины для категории: comp.windows.x\n",
      "the: 6749\n",
      "to: 3486\n",
      "and: 2447\n",
      "is: 2302\n",
      "of: 2296\n",
      "in: 1849\n",
      "for: 1394\n",
      "it: 1384\n",
      "on: 1143\n",
      "that: 1131\n",
      "thi: 1125\n",
      "you: 1066\n",
      "be: 1015\n",
      "use: 988\n",
      "are: 904\n",
      "if: 836\n",
      "with: 821\n",
      "or: 778\n",
      "file: 763\n",
      "not: 760\n",
      "\n",
      "Частые термины для категории: rec.sport.baseball\n",
      "the: 3507\n",
      "to: 1481\n",
      "and: 1312\n",
      "of: 1142\n",
      "in: 1116\n",
      "that: 883\n",
      "is: 876\n",
      "he: 738\n",
      "for: 580\n",
      "it: 561\n",
      "have: 529\n",
      "be: 510\n",
      "wa: 493\n",
      "but: 451\n",
      "you: 443\n",
      "on: 442\n",
      "thi: 416\n",
      "they: 409\n",
      "year: 405\n",
      "at: 404\n",
      "\n",
      "Частые термины для категории: rec.sport.hockey\n",
      "the: 5491\n",
      "to: 2045\n",
      "and: 1678\n",
      "in: 1621\n",
      "of: 1570\n",
      "that: 989\n",
      "is: 887\n",
      "for: 859\n",
      "it: 842\n",
      "he: 776\n",
      "wa: 765\n",
      "on: 725\n",
      "game: 680\n",
      "be: 643\n",
      "team: 635\n",
      "play: 564\n",
      "have: 550\n",
      "you: 550\n",
      "at: 547\n",
      "but: 532\n"
     ]
    }
   ],
   "source": [
    "vect_3 = CountVectorizer(max_features=10000)\n",
    "\n",
    "train_data = vect_3.fit_transform(stem_train)\n",
    "test_data = vect_3.transform(stem_test)\n",
    "\n",
    "x_3 = list(zip(vect_3.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "def SortbyTF(inputStr):\n",
    "    return inputStr[1]\n",
    "x_3.sort(key=SortbyTF, reverse = True)\n",
    "\n",
    "for _ in x_3[:20]:\n",
    "    print(_)\n",
    "\n",
    "category_names = twenty_train.target_names\n",
    "category_indices = twenty_train.target\n",
    "\n",
    "word_freq_pairs_3 = {}\n",
    "word_list_3 = {0:[], 1:[], 2:[]}\n",
    "\n",
    "for cat_idx, cat_name in enumerate(twenty_train.target_names):\n",
    "    print(f\"\\nЧастые термины для категории: {cat_name}\")\n",
    "\n",
    "    cat_mask = (category_indices == cat_idx)\n",
    "    cat_data = train_data[cat_mask]\n",
    "    \n",
    "    word_frequencies = np.ravel(cat_data.sum(axis=0))\n",
    "    word_freq_pairs_3[cat_idx] = list(zip(vect_3.get_feature_names_out(), word_frequencies))\n",
    "    word_freq_pairs_3[cat_idx].sort(key=SortbyTF, reverse=True)\n",
    "    \n",
    "    for word, freq in word_freq_pairs_3[cat_idx]:\n",
    "        if freq != 0:\n",
    "            word_list_3[cat_idx].append(word)\n",
    "\n",
    "    for word, freq in word_freq_pairs_3[cat_idx][:20]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Жаккар между comp.windows.x и rec.sport.baseball: 0.3481897478790775\n",
      "Жаккар между comp.windows.x и rec.sport.hockey: 0.3608225108225108\n",
      "Жаккар между rec.sport.baseball и rec.sport.hockey: 0.4852550478214665\n",
      "Отсечение стоп-слов\n",
      "Жаккар между comp.windows.x и rec.sport.baseball: 0.3201590553078684\n",
      "Жаккар между comp.windows.x и rec.sport.hockey: 0.3326839826839827\n",
      "Жаккар между rec.sport.baseball и rec.sport.hockey: 0.46455834242093785\n",
      "Стемминг и стоп-слова\n",
      "Жаккар между comp.windows.x и rec.sport.baseball: 0.32325903538597156\n",
      "Жаккар между comp.windows.x и rec.sport.hockey: 0.3147885014755711\n",
      "Жаккар между rec.sport.baseball и rec.sport.hockey: 0.44285714285714284\n"
     ]
    }
   ],
   "source": [
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[1]}:\", jaccard_similarity(word_list_1[0], word_list_1[1]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_1[0], word_list_1[2]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[1]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_1[1], word_list_1[2]))\n",
    "print('Отсечение стоп-слов')\n",
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[1]}:\", jaccard_similarity(word_list_2[0], word_list_2[1]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_2[0], word_list_2[2]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[1]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_2[1], word_list_2[2]))\n",
    "print('Стемминг и стоп-слова')\n",
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[1]}:\", jaccard_similarity(word_list_3[0], word_list_3[1]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[0]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_3[0], word_list_3[2]))\n",
    "print(f\"Жаккар между {twenty_train.target_names[1]} и {twenty_train.target_names[2]}:\", jaccard_similarity(word_list_3[1], word_list_3[2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный Байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(text_clf, y_train, y_test):\n",
    "    text_clf = text_clf.fit(y_train, twenty_train.target)\n",
    "    prediction = text_clf.predict(y_test)\n",
    "\n",
    "    y_test = twenty_test.target\n",
    "    print ('Prediction and test: ')\n",
    "    print (prediction)\n",
    "    print (y_test)\n",
    "    print ('Confusion matrix: ')\n",
    "    print (confusion_matrix(y_test, prediction))\n",
    "    print ('Accuracy score: ', accuracy_score(prediction, y_test))\n",
    "    print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 2 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[363  18  14]\n",
      " [  9 350  38]\n",
      " [  3  37 359]]\n",
      "Accuracy score:  0.9000839630562553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       395\n",
      "           1       0.86      0.88      0.87       397\n",
      "           2       0.87      0.90      0.89       399\n",
      "\n",
      "    accuracy                           0.90      1191\n",
      "   macro avg       0.90      0.90      0.90      1191\n",
      "weighted avg       0.90      0.90      0.90      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_dense = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n",
    "\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, twenty_train.data, twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[357  26  12]\n",
      " [  7 352  38]\n",
      " [  1  45 353]]\n",
      "Accuracy score:  0.8916876574307305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94       395\n",
      "           1       0.83      0.89      0.86       397\n",
      "           2       0.88      0.88      0.88       399\n",
      "\n",
      "    accuracy                           0.89      1191\n",
      "   macro avg       0.90      0.89      0.89      1191\n",
      "weighted avg       0.90      0.89      0.89      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[359  26  10]\n",
      " [  7 352  38]\n",
      " [  4  46 349]]\n",
      "Accuracy score:  0.8900083963056256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       395\n",
      "           1       0.83      0.89      0.86       397\n",
      "           2       0.88      0.87      0.88       399\n",
      "\n",
      "    accuracy                           0.89      1191\n",
      "   macro avg       0.89      0.89      0.89      1191\n",
      "weighted avg       0.89      0.89      0.89      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[364  22   9]\n",
      " [  8 350  39]\n",
      " [  4  37 358]]\n",
      "Accuracy score:  0.9000839630562553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       395\n",
      "           1       0.86      0.88      0.87       397\n",
      "           2       0.88      0.90      0.89       399\n",
      "\n",
      "    accuracy                           0.90      1191\n",
      "   macro avg       0.90      0.90      0.90      1191\n",
      "weighted avg       0.90      0.90      0.90      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[364  20  11]\n",
      " [ 11 333  53]\n",
      " [  4  41 354]]\n",
      "Accuracy score:  0.8824517212426533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       395\n",
      "           1       0.85      0.84      0.84       397\n",
      "           2       0.85      0.89      0.87       399\n",
      "\n",
      "    accuracy                           0.88      1191\n",
      "   macro avg       0.88      0.88      0.88      1191\n",
      "weighted avg       0.88      0.88      0.88      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[363  28   4]\n",
      " [ 61 303  33]\n",
      " [ 42 166 191]]\n",
      "Accuracy score:  0.7195633921074727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84       395\n",
      "           1       0.61      0.76      0.68       397\n",
      "           2       0.84      0.48      0.61       399\n",
      "\n",
      "    accuracy                           0.72      1191\n",
      "   macro avg       0.74      0.72      0.71      1191\n",
      "weighted avg       0.74      0.72      0.71      1191\n",
      "\n",
      "max features: 1000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[359  31   5]\n",
      " [  6 364  27]\n",
      " [  2  79 318]]\n",
      "Accuracy score:  0.8740554156171285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94       395\n",
      "           1       0.77      0.92      0.84       397\n",
      "           2       0.91      0.80      0.85       399\n",
      "\n",
      "    accuracy                           0.87      1191\n",
      "   macro avg       0.88      0.87      0.88      1191\n",
      "weighted avg       0.88      0.87      0.88      1191\n",
      "\n",
      "max features: 5000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[369  20   6]\n",
      " [  6 353  38]\n",
      " [  3  34 362]]\n",
      "Accuracy score:  0.910159529806885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       395\n",
      "           1       0.87      0.89      0.88       397\n",
      "           2       0.89      0.91      0.90       399\n",
      "\n",
      "    accuracy                           0.91      1191\n",
      "   macro avg       0.91      0.91      0.91      1191\n",
      "weighted avg       0.91      0.91      0.91      1191\n",
      "\n",
      "max features: 10000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[364  22   9]\n",
      " [  8 350  39]\n",
      " [  4  37 358]]\n",
      "Accuracy score:  0.9000839630562553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       395\n",
      "           1       0.86      0.88      0.87       397\n",
      "           2       0.88      0.90      0.89       399\n",
      "\n",
      "    accuracy                           0.90      1191\n",
      "   macro avg       0.90      0.90      0.90      1191\n",
      "weighted avg       0.90      0.90      0.90      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat = [100, 1000, 5000, 10000]\n",
    "for f in feat:\n",
    "    print(f'max features: {f}')\n",
    "    text_clf = Pipeline([\n",
    "                    ('vect', CountVectorizer(max_features= f, stop_words='english')),\n",
    "                    ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                    ('to_dense', to_dense),\n",
    "                    ('clf', GaussianNB()),\n",
    "                ])   \n",
    "\n",
    "    fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[369  20   6]\n",
      " [  6 353  38]\n",
      " [  3  34 362]]\n",
      "Accuracy score:  0.910159529806885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95       395\n",
      "           1       0.87      0.89      0.88       397\n",
      "           2       0.89      0.91      0.90       399\n",
      "\n",
      "    accuracy                           0.91      1191\n",
      "   macro avg       0.91      0.91      0.91      1191\n",
      "weighted avg       0.91      0.91      0.91      1191\n",
      "\n",
      "Длина словаря: 17254\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 5000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)\n",
    "\n",
    "vect_4 = CountVectorizer(stop_words='english')\n",
    "\n",
    "train_data = vect_4.fit_transform(stem_train)\n",
    "test_data = vect_4.transform(stem_test)\n",
    "print(f'Длина словаря: {len(vect_4.get_feature_names_out())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый вариант:\n",
    "- стемминг\n",
    "- отсекая стоп-слова\n",
    "- TF\n",
    "- max_features: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[364  21  10]\n",
      " [ 11 351  35]\n",
      " [  4  40 355]]\n",
      "Accuracy score:  0.8984047019311503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       395\n",
      "           1       0.85      0.88      0.87       397\n",
      "           2       0.89      0.89      0.89       399\n",
      "\n",
      "    accuracy                           0.90      1191\n",
      "   macro avg       0.90      0.90      0.90      1191\n",
      "weighted avg       0.90      0.90      0.90      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', GaussianNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мультиноминальный Наивный Байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[376  10   9]\n",
      " [ 12 364  21]\n",
      " [  5  74 320]]\n",
      "Accuracy score:  0.8900083963056256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       395\n",
      "           1       0.81      0.92      0.86       397\n",
      "           2       0.91      0.80      0.85       399\n",
      "\n",
      "    accuracy                           0.89      1191\n",
      "   macro avg       0.89      0.89      0.89      1191\n",
      "weighted avg       0.89      0.89      0.89      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, twenty_train.data, twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[378  10   7]\n",
      " [ 12 362  23]\n",
      " [  4  76 319]]\n",
      "Accuracy score:  0.889168765743073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.81      0.91      0.86       397\n",
      "           2       0.91      0.80      0.85       399\n",
      "\n",
      "    accuracy                           0.89      1191\n",
      "   macro avg       0.89      0.89      0.89      1191\n",
      "weighted avg       0.89      0.89      0.89      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[380   8   7]\n",
      " [ 13 358  26]\n",
      " [  2  46 351]]\n",
      "Accuracy score:  0.9143576826196473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.87      0.90      0.89       397\n",
      "           2       0.91      0.88      0.90       399\n",
      "\n",
      "    accuracy                           0.91      1191\n",
      "   macro avg       0.92      0.91      0.91      1191\n",
      "weighted avg       0.91      0.91      0.91      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[379   4  12]\n",
      " [ 15 332  50]\n",
      " [  3  15 381]]\n",
      "Accuracy score:  0.9168765743073047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       395\n",
      "           1       0.95      0.84      0.89       397\n",
      "           2       0.86      0.95      0.90       399\n",
      "\n",
      "    accuracy                           0.92      1191\n",
      "   macro avg       0.92      0.92      0.92      1191\n",
      "weighted avg       0.92      0.92      0.92      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = False)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[379   5  11]\n",
      " [ 12 343  42]\n",
      " [  4  12 383]]\n",
      "Accuracy score:  0.927791771620487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.95      0.86      0.91       397\n",
      "           2       0.88      0.96      0.92       399\n",
      "\n",
      "    accuracy                           0.93      1191\n",
      "   macro avg       0.93      0.93      0.93      1191\n",
      "weighted avg       0.93      0.93      0.93      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max features: 100\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[351  22  22]\n",
      " [ 29 270  98]\n",
      " [ 16 109 274]]\n",
      "Accuracy score:  0.7514693534844669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       395\n",
      "           1       0.67      0.68      0.68       397\n",
      "           2       0.70      0.69      0.69       399\n",
      "\n",
      "    accuracy                           0.75      1191\n",
      "   macro avg       0.75      0.75      0.75      1191\n",
      "weighted avg       0.75      0.75      0.75      1191\n",
      "\n",
      "max features: 1000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[373  13   9]\n",
      " [ 14 331  52]\n",
      " [  2  44 353]]\n",
      "Accuracy score:  0.887489504617968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       395\n",
      "           1       0.85      0.83      0.84       397\n",
      "           2       0.85      0.88      0.87       399\n",
      "\n",
      "    accuracy                           0.89      1191\n",
      "   macro avg       0.89      0.89      0.89      1191\n",
      "weighted avg       0.89      0.89      0.89      1191\n",
      "\n",
      "max features: 5000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[378   4  13]\n",
      " [ 13 345  39]\n",
      " [  3  18 378]]\n",
      "Accuracy score:  0.924433249370277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.94      0.87      0.90       397\n",
      "           2       0.88      0.95      0.91       399\n",
      "\n",
      "    accuracy                           0.92      1191\n",
      "   macro avg       0.93      0.92      0.92      1191\n",
      "weighted avg       0.93      0.92      0.92      1191\n",
      "\n",
      "max features: 10000\n",
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[379   5  11]\n",
      " [ 12 343  42]\n",
      " [  4  12 383]]\n",
      "Accuracy score:  0.927791771620487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.95      0.86      0.91       397\n",
      "           2       0.88      0.96      0.92       399\n",
      "\n",
      "    accuracy                           0.93      1191\n",
      "   macro avg       0.93      0.93      0.93      1191\n",
      "weighted avg       0.93      0.93      0.93      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat = [100, 1000, 5000, 10000]\n",
    "for f in feat:\n",
    "    print(f'max features: {f}')\n",
    "    text_clf = Pipeline([\n",
    "                    ('vect', CountVectorizer(max_features= f, stop_words='english')),\n",
    "                    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                    ('to_dense', to_dense),\n",
    "                    ('clf', MultinomialNB()),\n",
    "                ])   \n",
    "\n",
    "    fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[379   5  11]\n",
      " [ 12 337  48]\n",
      " [  3   9 387]]\n",
      "Accuracy score:  0.926112510495382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.96      0.85      0.90       397\n",
      "           2       0.87      0.97      0.92       399\n",
      "\n",
      "    accuracy                           0.93      1191\n",
      "   macro avg       0.93      0.93      0.93      1191\n",
      "weighted avg       0.93      0.93      0.93      1191\n",
      "\n",
      "Длина словаря: 17254\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)\n",
    "\n",
    "vect_5 = CountVectorizer(stop_words='english')\n",
    "\n",
    "train_data = vect_5.fit_transform(stem_train)\n",
    "test_data = vect_5.transform(stem_test)\n",
    "print(f'Длина словаря: {len(vect_5.get_feature_names_out())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый вариант:\n",
    "- стемминг\n",
    "- отсекая стоп-слова\n",
    "- TF-IDF\n",
    "- max_features: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and test: \n",
      "[0 1 1 ... 1 0 2]\n",
      "[0 1 1 ... 1 0 2]\n",
      "Confusion matrix: \n",
      "[[379   5  11]\n",
      " [ 12 343  42]\n",
      " [  4  12 383]]\n",
      "Accuracy score:  0.927791771620487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       395\n",
      "           1       0.95      0.86      0.91       397\n",
      "           2       0.88      0.96      0.92       399\n",
      "\n",
      "    accuracy                           0.93      1191\n",
      "   macro avg       0.93      0.93      0.93      1191\n",
      "weighted avg       0.93      0.93      0.93      1191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                ('vect', CountVectorizer(max_features= 10000, stop_words='english')),\n",
    "                ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                ('to_dense', to_dense),\n",
    "                ('clf', MultinomialNB()),\n",
    "            ])   \n",
    "\n",
    "fit_model(text_clf, stem_train, stem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Влияние этапов:\n",
    "- Стемминг - должен объединить схожие термины не сильно уменьшая информативность словаря. Уменьшает кол-во уникальных слов -> умменьшение словаря, повышает эффективность обработки. Но также могут появляться ошибки при обработки слов с одинаковым стеммом, но разным смыслом.\n",
    "- Отсечение стоп-слов - исключить самые часто встречающиеся термины (союзы, предлоги, частицы и т.п.) и слишком редко встречающиеся слова. Уменьшение размера словаря, анализ более значимых слов в тексте. Появление смысловых ошибок.\n",
    "- Взешивание:\n",
    "    - Логическое - простая модель анализирующая все слова встречающиеся в тексте. Но часто встречающиеся слова могут доминировать даже если они не важны. Не учитывается длина текста. Не учитываются схожие слова.\n",
    "    - TF - учитывает частоту вхождения слова в текст. Учитывается длина текста. \n",
    "    - IDF - уменьшает вес широкоупотребляемых слов в различных документах и повышает значимость тематических слов в конкретном тексте.\n",
    "- Максимальное кол-во терминов:\n",
    "    - Малое кол-во терминов упрощает модель, снижает переобучение. Теряются редкие тематические термины, плохое различие классов. \n",
    "    - Большое кол0во терминов учитывает больше уникальных слов, что помогает избежать потери информации. Лучше подходит для сложынх тематических текстов. Но усложняет модель, возможно переобучение, появляется шум."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
